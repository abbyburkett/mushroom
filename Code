#Import all necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#set the link as the url to read the data
url = "https://archive.ics.uci.edu/static/public/73/data.csv"

#Read the data
mushroom = pd.read_csv(url)

#Display the head of the data
mushroom.head()

#Display the shape of the data
mushroom.shape

#Display the data types of the variables
mushroom.info()

#Data Cleaning and Preprocessing
# Check for missing values
missing_values = mushroom.isnull().sum()
missing_values

# Drop rows with missing data in the 'stalk-root' column
mushroom_cleaned = mushroom.dropna(subset=['stalk-root'])
mushroom_cleaned.shape

#Display dataset after cleaning
mushroom_cleaned.head()

categorical_columns = mushroom_cleaned.select_dtypes(include=['object']).columns
# Create a copy of the dataset for encoding
mushroom_encoded = mushroom_cleaned.copy()

# Convert each categorical variable into numeric categories using factorize
for col in categorical_columns:
    mushroom_encoded[col] = pd.factorize(mushroom_encoded[col])[0]

#Display dataset after categorization
mushroom_encoded.head()

# Check the new data types to ensure they are now numeric
mushroom_encoded.dtypes

# List the numeric columns for visualization
numeric_columns = mushroom_encoded.select_dtypes(include=['int', 'float']).columns

# Create box plots to visualize outliers
for column in numeric_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=mushroom_encoded[column])
    plt.title(f"Box Plot for {column}")
    plt.show()

# Calculate the interquartile range for numerical columns
selected_columns = ['cap-color','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-surface-above-ring',
                    'stalk-color-above-ring','stalk-color-below-ring','veil-color','ring-number','ring-type']
Q1 = mushroom_encoded[selected_columns].quantile(0.25)
Q3 = mushroom_encoded[selected_columns].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers based on the defined bounds
mushroom_no_outliers = mushroom_encoded[~((mushroom_encoded[selected_columns] < lower_bound) | (mushroom_encoded[selected_columns] > upper_bound)).any(axis=1)]

#Check shape of dataset without outliers
mushroom_no_outliers.shape

#Display dataset with no outliers
mushroom_no_outliers.head()

#Visualization
# Get the list of all columns except the target variable 'poisonous'
features = [col for col in mushroom_no_outliers.columns if col != 'poisonous']

# Set up the figure and axis grid
fig, axes = plt.subplots(len(features), 1, figsize=(8, 5 * len(features)))

# Iterate through each feature and plot the histogram
for i, feature in enumerate(features):
    sns.histplot(x=feature, data=mushroom_no_outliers, stat='density', kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {feature}')
    axes[i].set_xlabel('')
    axes[i].set_ylabel('Density')

plt.tight_layout()
plt.show()

# Create count plots for all variables with hue set to the target variable (poisonous)
plt.figure(figsize=(20, 15))
for i, column in enumerate(mushroom_no_outliers.columns):
    plt.subplot(5, 5, i + 1)
    sns.countplot(x=column, hue='poisonous', data=mushroom_no_outliers)
    plt.title(f"Count Plot for {column}")
    plt.xlabel("")
plt.tight_layout()
plt.show()

# Visualize the correlation between variables using a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(mushroom_no_outliers.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

#Train/Test split and Normalization/Standardization
X = mushroom_no_outliers.drop('poisonous', axis=1)
y = mushroom_no_outliers['poisonous']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the data using StandardScaler
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()



X_train_scaled = scaler.fit_transform(X_train)


X_test_scaled = scaler.transform(X_test)

# Fit the logistic regression model to the training data
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression( max_iter=2000)

lr.fit(X_train_scaled, y_train)

#Build a base regression model and train/test, including all features
#Logistic Regression
# Evaluate the performance of the model on the test data
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, recall_score, precision_score, roc_auc_score, roc_curve

# Evaluate the performance of the model on the test data
y_pred = lr.predict(X_test_scaled)


# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
true_negatives = conf_matrix[0, 0]
false_negatives = conf_matrix[1, 0]
true_positives = conf_matrix[1, 1]
false_positives = conf_matrix[0, 1]
conf_matrix

# Calculate sensitivity and specificity
sensitivity = true_positives / (true_positives + false_negatives)
specificity = true_negatives / (true_negatives + false_positives)

# Predict probabilities for AUC calculation
y_pred_proba = lr.predict_proba(X_test_scaled)[:, 1]

# Calculate AUC
auc = roc_auc_score(y_test, y_pred_proba)

accuracy_base = accuracy_score(y_test, y_pred)
f1_base = f1_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Display performance metrics
print(f"Accuracy: {accuracy_base}")
print(f"f1-score: {f1_base}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"Sensitivity: {sensitivity}")
print(f"Specificity: {specificity}")
print(f"AUC: {auc}")

print(classification_report(y_test, y_pred))

# Plot the confusion matrix
plt.figure(figsize=(7, 7))
sns.heatmap(conf_matrix,
            annot=True,
            fmt=".2f",
            linewidths=.5,
            square=True,
            cmap='Blues_r')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.title(f'Confusion Matrix (Logistic Regression), Accuracy: {accuracy_base:.4f}')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)  # Define fpr, tpr, and _ before plotting
plt.figure()
plt.plot(fpr, tpr, label=f'Logistic Regression (AUC={auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

#Decision Tree
from sklearn.model_selection import train_test_split

# Assume 'mushroom_no_outliers' is your cleaned dataset
X = mushroom_no_outliers.drop('poisonous', axis=1)  # Features (all but the target)
y = mushroom_no_outliers['poisonous']  # Target variable (poisonous or edible)

# Split the data into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.tree import DecisionTreeClassifier, export_graphviz

# Build and train a Decision Tree model
dt = DecisionTreeClassifier(max_depth=5)  # Example constraint to prevent overfitting
dt.fit(X_train, y_train)  # Train the model on the training set

# Export the decision tree to a Graphviz format
dot_data = export_graphviz(dt,
                           out_file=None,
                           feature_names=X.columns,
                           class_names=str(y.unique()),
                           filled=True,
                           rounded=False)

import graphviz

# Display the decision tree using Graphviz and set the size
graph = graphviz.Source(dot_data)
#graph.format = 'png'
graph.render('decision_tree', view=True, format='png', cleanup=True)
#graph.render('decision_tree', view=True, format='pdf', cleanup=True)

graph

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    roc_auc_score,
    classification_report,
)

# Make predictions on the test set
y_pred = dt.predict(X_test)

# Predict probabilities for AUC and ROC curve
y_pred_proba = dt.predict_proba(X_test)[:, 1]  # Get probabilities for positive class

# Calculate various performance metrics
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

# Confusion Matrix to calculate sensitivity and specificity
conf_matrix = confusion_matrix(y_test, y_pred)

true_negatives = conf_matrix[0, 0]
false_negatives = conf_matrix[1, 0]
true_positives = conf_matrix[1, 1]
false_positives = conf_matrix[0, 1]

sensitivity = true_positives / (true_positives + false_negatives)  # Sensitivity
specificity = true_negatives / (true_negatives + false_positives)  # Specificity

# Calculate AUC
auc = roc_auc_score(y_test, y_pred_proba)

# Output performance metrics
print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Precision:", precision)
print("Recall:", recall)
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)
print("AUC:", auc)

# Generate a detailed classification report
print(classification_report(y_test, y_pred))

#Select the best set of input features
#Logistic Regression
# recursive feature elimination
from sklearn.feature_selection import RFE

K = X.shape[1]

accuracy = []
f1 = []

for i in range(1, K + 1):
  # Out of 14 x input features select k
  rfe = RFE(estimator=LogisticRegression(), n_features_to_select=i)
  # fit the RFE object to the data
  rfe.fit(X_train_scaled, y_train)

  # select only the selected features
  X_selected = X[X.columns[rfe.support_]]

  X_selected_train, X_selected_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.2, random_state=42
    )

  logreg = LogisticRegression(solver='saga', max_iter=3000)
  logreg.fit(X_selected_train, y_train)

  y_pred = logreg.predict(X_selected_test)

  accuracy.append(accuracy_score(y_test, y_pred))

  f1.append(f1_score(y_test, y_pred))

plt.plot(range(1, K + 1), accuracy, label='Accuracy')

plt.plot(range(1, K + 1), f1, label='F1-Score')

metric_dict = {'k': range(1, K + 1), 'Accuracy': accuracy, 'F1-Score': f1}

metric_df = pd.DataFrame(metric_dict)
metric_df

metric_df[(metric_df['Accuracy'] == max(metric_df['Accuracy']))| (metric_df['F1-Score'] == max(metric_df['F1-Score']))]

#Build the Regression Model from k-best features
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix,
    f1_score,
    accuracy_score,
    roc_auc_score,
    roc_curve,
    precision_score,
    recall_score,
)

# the best model so far is with k=1, or all features
rfe = RFE(estimator=LogisticRegression(), n_features_to_select=1)
rfe.fit(X_train_scaled, y_train)  # Fit RFE to the training data

# Select the best features
X_selected = X_train_scaled[:, rfe.support_]
X_selected_test = X_test_scaled[:, rfe.support_]

# Build and train Logistic Regression model with selected features
lr = LogisticRegression(solver='saga', max_iter=3000)
lr.fit(X_selected, y_train)

# Make predictions and predict probabilities
y_pred = lr.predict(X_selected_test)
y_proba = lr.predict_proba(X_selected_test)[:, 1]

# Evaluate the performance of the model on the test data
cm2=confusion_matrix(y_test, y_pred)
f12= f1_score(y_test, y_pred, average='weighted')
accuracy2=accuracy_score(y_test, y_pred)
precision2=precision_score(y_test, y_pred)
recall2=recall_score(y_test, y_pred)
auc2 = roc_auc_score(y_test, y_proba)

true_negatives = cm2[0, 0]
false_negatives = cm2[1, 0]
true_positives = cm2[1, 1]
false_positives = cm2[0, 1]

sensitivity2 = true_positives / (true_positives + false_negatives)
specificity2 = true_negatives / (true_negatives + false_positives)

print(cm2)
print(f"F1-Score: {f12}")
print(f"Accuracy: {accuracy2}")
print(f"Precision: {precision2}")
print(f"Recall: {recall2}")
print(f"AUC: {auc2}")
print(f"Sensitivity: {sensitivity2}")
print(f"Specificity: {specificity2}")

# plot the ROC curve
plt.plot(fpr, tpr, label=f'Logistic Regression (AUC={auc2:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

#Summarize your findings
# Evaluation metrics for the base model
print("Base Model Metrics:")
print(f"Accuracy:", accuracy_base)
print(f"f1-score:", f1_base)
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"Sensitivity: {sensitivity}")
print(f"Specificity: {specificity}")
print(f"AUC: {auc}")

# Evaluation metrics for the model with k-best features
print("\nModel with K-best Features Metrics:")
print(f"Accuracy: {accuracy2}")
print(f"f1-score: {f12}")
print(f"Precision: {precision2}")
print(f"Recall: {recall2}")
print(f"Sensitivity: {sensitivity2}")
print(f"Specificity: {specificity2}")
print(f"AUC: {auc2}")
